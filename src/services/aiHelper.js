const OpenAI = require('openai');
require('dotenv').config();

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const VECTOR_STORE_ID = process.env.VECTOR_STORE_ID;
const assistantCache = {};

function toText(value) {
    if (value == null) return '';
    if (typeof value === 'string') return value;
    try { return JSON.stringify(value, null, 2); } catch { return String(value); }
}

function combinarContextos({ ragContext, userProfile, weatherData, pergunta, baseInstructions, dadosUso }) {
    const ctx = [];

    // 0. Base Instructions (Persona)
    if (baseInstructions) {
        ctx.push(`${baseInstructions}`);
    }

    // 1. Conhecimento Especializado (RAG)
    if (ragContext) {
        ctx.push(`CONHECIMENTO ESPECIALIZADO:\nUse a documenta√ß√£o especializada quando necess√°rio para responder com precis√£o t√©cnica sobre: ${ragContext}`);
    }

    // 2. Contexto Clim√°tico
    if (weatherData) {
        const temp = weatherData.temperature ?? '?';
        const hum = weatherData.humidity ?? '?';
        const desc = weatherData.weather?.description ?? 'N/A';
        const city = weatherData.city || 'Pelotas/RS';
        const hora = new Date().toLocaleTimeString('pt-BR');

        ctx.push(`CONTEXTO CLIM√ÅTICO ATUAL EM ${city.toUpperCase()}:\n- Temperatura: ${temp}¬∞C\n- Condi√ß√£o: ${desc}\n- Umidade: ${hum}%\n- Hora da consulta: ${hora}`);

        // Recomenda√ß√µes din√¢micas simples baseadas no clima
        let recClima = '‚Ä¢ Mantenha o conforto t√©rmico de forma passiva';
        if (temp > 25) recClima = '‚Ä¢ Aproveite a ventila√ß√£o natural; evite ganho de calor solar direto';
        if (temp < 15) recClima = '‚Ä¢ Maximize aquecimento solar passivo; evite correntes de ar frio';

        ctx.push(`RECOMENDA√á√ïES ESPEC√çFICAS PARA O CLIMA ATUAL:\n${recClima}`);
        ctx.push(`ZONA BIOCLIM√ÅTICA: ZB2 (${city} - Subtropical √ömido)\n- Estrat√©gias recomendadas: Ventila√ß√£o cruzada no ver√£o, aquecimento solar passivo no inverno`);
    }

    // 3. Personaliza√ß√£o Baseada no Usu√°rio
    if (userProfile || dadosUso) {
        const perfil = userProfile || 'Intermedi√°rio';
        const uso = dadosUso || {};
        const interactions = uso.totalInteracoes || 0;
        const periodo = uso.periodoPreferencial || 'vari√°vel';

        ctx.push(`PERSONALIZA√á√ÉO BASEADA NO USU√ÅRIO:\n- PERFIL: ${perfil}\n- HIST√ìRICO DE USO: ${uso.frequenciaUso || 'novo'}, interage principalmente no periodo da ${periodo}; ${interactions} intera√ß√µes registradas.\n- PILARES TCP ATIVOS: atitude, norma, controle`);

        // Guidelines de estrutura baseadas no perfil
        let estrutura = '';
        if (perfil === 'Proativo') {
            estrutura = '1. Cumprimente de forma adequada ao perfil Proativo\n2. Inclua benef√≠cio pessoal claro (econ√¥mico, conforto, ambiental)\n3. Adicione refer√™ncia social motivadora\n4. Sugira a√ß√£o simples e acess√≠vel para hoje\n5. Insira a dica com "üí°"\n6. Mencione influ√™ncia do clima\n7. Finalize com convite suave';
        } else if (perfil === 'Descuidado') {
            estrutura = '1. Use tom acolhedor e muito simples\n2. Foque apenas em economia financeira imediata\n3. Sugira uma √∫nica a√ß√£o extremamente f√°cil\n4. Reforce que "todo come√ßo importa"\n5. Evite qualquer tecnicismo';
        } else {
            estrutura = '1. Cumprimente com energia moderada\n2. Relacione conforto e economia\n3. Sugira a√ß√£o pr√°tica de m√©dio impacto\n4. Convite a experimentar novos h√°bitos';
        }
        ctx.push(`ESTRUTURA PERSONALIZADA DA RESPOSTA:\n${estrutura}`);
    }

    if (pergunta) {
        ctx.push(`INSTRU√á√ÉO FINAL:\nUse o conhecimento especializado acima para fundamentar sua resposta √† pergunta abaixo, adaptando a linguagem ao perfil ${userProfile || 'do usu√°rio'}.\n\nPERGUNTA DO USU√ÅRIO: ${pergunta}`);
    }

    return ctx.join('\n\n========================================\n\n');
}

const prompts = require('../config/prompts');

function buildBaseInstructionsEficiencia() {
    return prompts.assistants.eficiencia.instructions;
}

async function getOrCreateAssistantEficiencia() {
    const name = 'Efici√™ncia';
    if (assistantCache[name]) return assistantCache[name];

    const existing = await openai.beta.assistants.list();
    const found = existing.data.find(a => a.name === name);
    if (found) {
        assistantCache[name] = found.id;
        return found.id;
    }

    const created = await openai.beta.assistants.create({
        name,
        model: process.env.LLM_MODEL_EFICIENCIA || 'gpt-4o-mini',
        instructions: buildBaseInstructionsEficiencia(),
        tools: [{ type: 'file_search' }],
        tool_resources: VECTOR_STORE_ID ? { file_search: { vector_store_ids: [VECTOR_STORE_ID] } } : undefined,
    });

    assistantCache[name] = created.id;
    return created.id;
}

function buildBaseInstructionsVolts() {
    return prompts.assistants.volts.instructions;
}

async function getOrCreateAssistantVolts() {
    const name = 'Volts';
    if (assistantCache[name]) return assistantCache[name];

    const existing = await openai.beta.assistants.list();
    const found = existing.data.find(a => a.name === name);
    if (found) {
        assistantCache[name] = found.id;
        return found.id;
    }

    const created = await openai.beta.assistants.create({
        name,
        model: process.env.LLM_MODEL_VOLTS || 'gpt-4o-mini',
        instructions: buildBaseInstructionsVolts(),
        // Sem RAG (file_search) para Volts
    });

    assistantCache[name] = created.id;
    return created.id;
}

async function addMessageToThread(threadId, role, content) {
    return openai.beta.threads.messages.create(threadId, { role, content: toText(content) });
}

async function runAssistantOnThread(threadId, assistantId, systemPatch) {
    const run = await openai.beta.threads.runs.create(threadId, {
        assistant_id: assistantId,
        instructions: toText(systemPatch), // INJE√á√ÉO DO CONTEXTO DIN√ÇMICO por run
    });

    // Poll at√© completar (timeout simples)
    let status, attempts = 0;
    do {
        await new Promise(r => setTimeout(r, 1000));
        const r2 = await openai.beta.threads.runs.retrieve(threadId, run.id);
        status = r2.status;
        attempts++;
    } while (status !== 'completed' && status !== 'failed' && attempts < 60);

    if (status !== 'completed') throw new Error(`Run falhou: ${status}`);

    const list = await openai.beta.threads.messages.list(threadId);
    const msg = list.data.find(m => m.role === 'assistant') || list.data[0];
    const txt = msg?.content?.find?.(c => c.type === 'text')?.text?.value || '';
    return txt;
}

async function addMessageAndRunAssistant(threadId, userMessage, assistantId, systemPatch) {
    const msgText = toText(userMessage); // <-- garante string
    await addMessageToThread(threadId, 'user', msgText);
    return runAssistantOnThread(threadId, assistantId, systemPatch);
}

// =============================
// STREAMING
// =============================
function runAssistantOnThreadStream(threadId, assistantId, systemPatch) {
    return openai.beta.threads.runs.stream(threadId, {
        assistant_id: assistantId,
        instructions: toText(systemPatch),
    });
}

async function addMessageAndRunAssistantStream(threadId, userMessage, assistantId, systemPatch) {
    const msgText = toText(userMessage);
    await addMessageToThread(threadId, 'user', msgText);
    return runAssistantOnThreadStream(threadId, assistantId, systemPatch);
}

module.exports = {
    openai,
    toText,
    combinarContextos,
    getOrCreateAssistantEficiencia,
    addMessageToThread,
    runAssistantOnThread,
    addMessageToThread,
    runAssistantOnThread,
    addMessageAndRunAssistant,
    runAssistantOnThreadStream,
    addMessageAndRunAssistantStream,
    addMessageAndRunAssistantStream,
    getOrCreateAssistantVolts,
    getEficienciaInstructions: () => prompts.assistants.eficiencia.instructions
};
